<!DOCTYPE HTML>
<!--
	Mirror pioneer - CompSci by KyraZzz
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Machine learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="logo">
							<span class="icon fa-gem"></span>
						</div>
						<div class="content">
							<div class="inner">
								<h1>Implementation and Application of K-Means Clustering and Polynomial Regression</h1>
								<p>Copyright 2020 @WEIJUN HUANG. Not to be quoted or reproduced without permission from the Mirror education.</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#intro">Intro</a></li>
								<li><a href="#kms">K-Means Clustering</a></li>
								<li><a href="#linreg">Linear Regression</a></li>
								<li><a href="#conclusion">Reference</a></li>
								<!-- <li><a href="#elements">Elements</a></li> -->
								<li><a href="#code">Code</a></li>
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="intro">
								<h2 class="major">Intro</h2>
								<span class="image main">
									<img src="images/universe.jpg" alt="" />
								</span>
								<p>In the following paragraph, we will finally talk about the method of clustering data and the advantages and disadvantages it. After, we are going to talk about the implementation of the algorithm. At last, I implement our algorithm on the iris data set.<a href="#kms">K-Means Clustering</a>.</p>
							</article>

						<!-- kms -->
							<article id="kms">
								<h2 class="major">K-Means Clustering</h2>
								<span class="image main"><img src="images/pic02.jpg" alt="" /></span>
								<h1>1.Introduction</h1>
								<p>K ‚Äì means clustering is an unsupervised algorithm. This algorithm is to find how many groups needed for grouping data and how to group it. Its objective is to divide data into k groups and minimize the within-cluster sum of the squared difference(WCSS), which is the sum of squared difference between all the data to its corresponding means. In this way, we can have a grouping result that we are expected about. Mathematically, it can be expressed as n data where x = {x1 ,x2 ,‚Ä¶,xn } into K groups. Œº i is the mean value of each group. If Group S = { s1 ,s2 ,‚Ä¶,s n } , x ‚ààS, then</p>
								<span class="image main">
									<img src="images/1.png">
								</span>
								<p>In the following paragraph, we will finally talk about the method of clustering data and the advantages and disadvantages of it. After, we are going to talk about the implementation of the algorithm. At last, I implement our algorithm on the iris data set.</p>
								<h1>2.Algorithm</h1>
								<h3>2.1 Five Steps For Clustering</h3>
								<p>There are five steps for grouping.</p>
									<ol>
										<li>Initialize the coordinates of mean points{ u1 ,u2 ,‚Ä¶,u n }.</li>
										<li>Calculate the distance between all data x to each mean ui </li>
										<li>Assign data x to the group with the nearest mean.</li>
										<li>Recalculate the means based on the current assignment.</li>
										<li>At Last, repeat 2-4 until means converge.</li>
									</ol>
								<h3>2.2 3 random initializing method</h3>
								<p>There are three methods for initialization.</p>
								<p>The first method is to randomly initialize means within the maximum value and minimum value of the data. The second method is to choose data randomly as the mean point. The Third one is called k-means ++. It works with following steps</p>
								<ol>
									<li>it randomly selects one data as the first mean.</li>
									<li>it calculates the distance between data and its closet mean.</li>
									<li>it selects x as the next mean with the probability, where probability is proportional to the distance square. In this way, you can ensure that means are dispersed.</li>
									<li>Finally, keep doing 2,3 until k means selected.</li>
								</ol>
								<h3>2.3 determine k values</h3>
								<p>We also need to determine k before grouping. This can be done by ‚ÄúElbow Method‚Äù. First, we need to calculate WCSS by each k value by multiple times to get an average value of WCSS. Draw the graph of WCSS against K. A high or low k-value determines the significance of the clustering result.</p>
								<p>For example, the graph is shown in Figure 1, The graph looks like an inverse proportional function. The point of the elbow is at the value of k = 4, which has a relatively small WCSS and a reasonable number of classes. The reason for finding an elbow is because an elbow point is normally the expected group number so it appears a better result.</p>
								<span class="image main">
									<img src="images/2.png">
									<figcaption style="text-align: center; width: 100%;">Figure 1:The graph of WCSS against K. The point of elbow is the most optimum value of K</figcaption>
								</span>
								<h3>2.4 advantages & disadvantages</h3>
								<p>There are several advantages to this algorithm. First, it is an unsupervised learning algorithm, so it does not need the correct label for the input data. It can learn how to classify the data only from the information of data. Second, the algorithm is proved to converge mathematically. Third, it is easy to be implemented due to its simplicity.</p>
								<p>The algorithm looks reasonable, but it also has several limitations.</p>
								<p>In initialization, K-means++ has a low processing speed but an accurate result of clustering. However, the first two methods may sort data in a high efficiency but the final result may not correct. Apart from that, this algorithm is heavily dependent on initialization.</p>
								<p>Besides, distance sometimes is hard to define such as graph or matrix so impossible to determine k and for clustering. Furthermore, it is guaranteed to converge, but sometimes up in local minimum which not show the most optimum results of grouping due to the method of initialization used.</p>
								<h1>3.Code structure</h1>
								<h3>3.1Flow chart</h3>
								<span class="image main">
									<img src="images/3.png">
									<figcaption style="text-align: center; width: 100%;">Figure 2: The flow chart of the process</figcaption>
								</span>
								<h3>3.2explanation</h3>
								<p>This algorithm consists of three mean parts. Initialization, distance, and new mean. The first parts of initialization are to creating mean points. The initialization method I used is called Kmeans ++ [1] which is proposed by Arthur et al. It firstly chooses a data as mean randomly and then chooses other points according to the distance to the mean after that. The data with further distance will have a greater possibility to be selected. In this situation, the initial means can be spread out and reduce the probability of meeting a local minimum. The second module is for calculated the distance and assign to each group. The distance between each mean and each data is being calculated by using np.linalg.norm() and data will be assigned to the group with that the distance between them is the least. The last module is to recalculate the mean according to the current groups. This can be done by using np.sum() and the result of it is divided by the average data in this group. For plotting the graph, I initially drawing the initial data with a black dot. After that, each group of data is assigned to a different color and the mean is being assigned to an identical color.</p>
								<h3>3.3Results</h3>
								<p>Figure 3 shows the results of clustering the data when k = 4. In Figure 3, purple stands for means in each group, and blue, red, green, yellow stands for the data in various groups.</p>
								<span class="image main">
									<img src="images/4.png">
									<figcaption style="text-align: center; width: 100%;">Figure 3:Result of clustering</figcaption>
								</span>
								<h1>4.Iris data</h1>
								<h3>4.1introduction</h3>
								<p>The whole name of Iris data is called ‚ÄúAnderson‚Äôs Iris data set‚Äù. It consists of 150 data in 4 dimensions. Each data records four information of flower which is sepal length, sepal width, petal length, and petal width, an example is shown in Figure 4. [2]</p>
								<span class="image main">
									<img src="images/5.png">
									<figcaption style="text-align: center; width: 100%;">Figure 4:samples of data in this dataset</figcaption>
								</span>
								<h3>4.2implementation</h3>
								<p>The method of clustering this data is the same as the previous data. The only changes in this data are calculating 2 more dimensions when calculating WCSS, as well as calculating distance.</p>
								<h3>4.3result</h3>
								<p>From Figure 5, by calculating the WCSS and drawing a graph of WCSS against K, we can know that k=3 is the optimum choice for clustering data. After clustering, we can use any two dimensions to be the x-axis and y-axis to displaying the clustering results. Figure 6 shows the results of clustering by displaying different dimensions. However, the result is not correct when classifying this data because two types of flowers are almost mixed together, so it is almost impossible to separate them, which reveals k-mean clustering is not optimum for classifying in this dataset.</p>
								<span class="image main">
									<img src="images/6.png">
									<figcaption style="text-align: center; width: 100%;">Figure 5:The graph of WCSS against K. The point of the elbow is the most optimum value of K</figcaption>
								</span>
								<span class="image main">
									<img src="images/7.png">
									<figcaption style="text-align: center; width: 100%;">Figure 6:Result of clustering</figcaption>
								</span>
								<h1>5.Conclusion</h1>
								<p>We implement and test K-mean clustering on two datasets. one is synthetic data with separable classes, while another is an inseparable Iris Dataset. We proved that K-mean clustering performs better on a separable dataset.</p>
							</article>

						<!-- linreg -->
							<article id="linreg">
								<h2 class="major">Linear Regression</h2>
								<span class="image main"><img src="images/pic03.jpg" alt="" /></span>
								<h1>1.Introduction</h1>
								<p>polynomial regression is a fundamental supervised learning algorithm in machine learning, which means that training data contains both parameters and the corresponding value to it. Its standard form looks like y=w0+ w1x1+w2x2+w3x3+‚ãØ+wkxk . The goal of this algorithm is to fit the best hyperplane for the given training data by minimizing the loss function. i.e. find the best w0,w1,‚Ä¶‚Ä¶wk. The mathematical formula is showing below, where N is the number of data, W and X are parameter and data in matrix respectively. a stands for learning rate to adjust the speed of converging. T means transposing the matrix. aùëäTùëä is regularization which restricts the size of L which stands for loss function. The loss function can let us know whether it is close to the exact result.</p>
								<span class="image main">
									<img src="images/8.png">
								</span>
								<p>In the following paragraph, we will briefly talk about the algorithm of polynomial regression, and showing how it is being proved by a mathematical formula. We will also compare the different algorithm that has been used. After that, we are going to talk about the implementation of this method. At last, the result will be shown.</p>
								<h1>2.Algorithm</h1>
								<h3>2.1Gradient Descent</h3>
								<p>The first method for finding the best fitting line is gradient descent. This method is to calculate the next value of W from the previous value of W minus by the differentiation of L. The mathematical formula by calculating this is shown below, where a is the learning rate that has been mentioned before, and Y is the expected result of data.</p>
								<span class="image main">
									<img src="images/9.png">
								</span>
								<p>The process of algebra manipulation is shown below.</p>
								<span class="image main">
									<img src="images/11.png">
								</span>
								<h3>2.2Calculate global minimum directly</h3>
								<p>The second method is to calculate the global minimum directly [3]. It is a feasible method because the loss function is a convex function, so there is only one minimum point. This ef method is to calculate out value W as = 0. The formula is showing like this:</p>
								<span class="image main">
									<img src="images/12.png">
								</span>
								<p>Then I will show how to prove it. When using dL/dw=0 for calculating the global minimum</p>
								<span class="image main">
									<img src="images/13.png">
								</span>
								<h3>2.3The comparison between them</h3>
								<p>There are some similarities between them.</p>
								<ol>
									<li>they both use to estimate a function by given training data. After that, they will both estimate the value of validation.</li>
									<li>They have the same aim that they want to minimize the sum of the square difference between an estimated value and a real value.</li>
									<li>They can both guarantee to find a minimum point.</li>
								</ol>
								<p>However, they have some differences.</p>
								<p>First of all, calculate the global minimum method find the minimum point by differentiation directly instead of iteration. As a result, this method requires the presence of differentiation and a singular matrix, so not all loss functions can be guaranteed to be calculated. However, it has a high processing speed than gradient descent.</p>
								<p>In gradient descent, it iterates until converges when finding a minimum point. The disadvantage of it is the speed of calculation is slow. It may also not guarantee to converge because it may meet gradient explosion due to the value of the gradient is too big. To solve this problem, we can set the learning rate to slow down the iteration speed. Although this method is not optimum for polynomial regression, it is widely used in linear regression, because it can be used to calculating almost all the loss functions.</p>
								<h1>3.Implementation</h1>
								<p>We implement the code by using NumPy library. The structure of the code is shown in Figure 7.</p>
								<span class="image main">
									<img src="images/14.png">
									<figcaption style="text-align: center; width: 100%;">Figure 7:The diagram illustrates the overall structure of the code. It consists of three parts: reading data, calculation and plotting.</figcaption>
								</span>
								<p>Our code consists of three parts. The first part is reading data from the file. The data will be divided into training data and testing data. Some useful variables will also be initialized. The second parts consist of two options to obtain W. The first method is using gradient descent to obtain value when converge. The second method is using Least-squares estimation for calculating out directly. The third part is to draw the graph. I draw two kinds of graphs. The first graph is the estimated data and the second graph is the loss value in each number of parameters.</p>
								<h1>4.Result</h1>
								<p>I implement my test in polynomial regression rather than linear regression. Polynomial regression uses x , ùë• 1 , ùë• l , ùë• m , ‚Ä¶ ‚Ä¶ , ùë• n rather than x 8 , x 1 , x l , x m , ‚Ä¶ ‚Ä¶ , x n that is used in linear regression for several dimensions. As a result, polynomial regression only has two dimensions which show its simplicity. Apart from that, linear regression is more optimum for the data that looks linear and polynomial regression can fits more with the data that is in a normal distribution.</p>
								<p>Figure 10 shows the change in the graphic of function when the number of parameters changes. Figure 9shows the relationship between training data and relative function when the number of parameters is 11. We can easily see that the function in 12 parameter changes a lot which leads to a high loss in testing data that is shown in Figure 8. From watching the loss function graph, 11 parameters are the most optimum value for doing linear regression where training loss and testing loss are both very low. Apart from that, by using regularization, we can see that the loss function of testing data has been decreased more significantly at the optimum value of 1e-6 which shown by Figure 11. However, a large regularization may not show a very good result which is shown in Figure 12.</p>
								<span class="image main">
									<img src="images/15.png">
									<figcaption style="text-align: center; width: 100%;">Figure 10: The result function when the number of parameters is 9, 10 ,11 and 12 respectively with testing data</figcaption>
								</span>
								<span class="image main">
									<img src="images/16.png">
									<figcaption style="text-align: center; width: 100%;">Figure 9: :function and testing data when the parameter is 11</figcaption>
								</span>
								<span class="image main">
									<img src="images/17.png">
									<figcaption style="text-align: center; width: 100%;">Figure 8:loss function graph of testing data(yellow line) and training data(blue line)</figcaption>
								</span>
								<span class="image main">
									<img src="images/18.png">
									<figcaption style="text-align: center; width: 100%;">Figure 12: regularization by setting value in 1e-6</figcaption>
								</span>
								<span class="image main">
									<img src="images/19.png">
									<figcaption style="text-align: center; width: 100%;">Figure 11: regularization by setting value in 2e-6</figcaption>
								</span>
								<h1>5.Conclusion</h1>
								<p>We implement and test polynomial regression on this dataset and proved that polynomial regression worked well on this dataset with the help of regularization.</p>






							</article>

						<!-- conclusion -->
							<article id="conclusion">
								<h2 class="major">Reference</h2>
								<span class="image main"><img src="images/pic03.jpg" alt="" /></span>
								<p>[1] D. Arthur and S. Vassilvitskii, K-means++:the advantages of careful seeding, proceeding of the enghteenth annual ACM-SIAM symposium on Discrete algorithms, 2007.</p>

								<p>[2] "wikipedia.org," 2021. [Online]. Available: https://en.wikipedia.org/wiki/Iris_flower_data_set.</p>
									
								<p>[3] "Wikipedia," 2021. [Online]. Available: https://en.wikipedia.org/wiki/Linear_regression.</p>
							</article>


							<article id="code">
								<h2 class="major">Code</h2>
								<span class="image main"><img src="images/pic03.jpg" alt="" /></span>
								<h1>k-means clustering</h1>
								<pre><code>
									#12131232131231
import numpy as np
from random import *
import matplotlib.pyplot as plt
import pandas as pd
class k_mean():
    def __init__(self):
        self.first = True
        #2dimen
        # self.data = np.load('Êú∫Âô®Â≠¶‰π†/clustering_data.npy')
        # self.data = self.data["data"]
        #4dimen
        self.data = pd.read_csv('/Users/huangweijun/Library/Mobile Documents/com~apple~CloudDocs/ÁºñÁ®ã/P2_CLASS/Êú∫Âô®Â≠¶‰π†/k-mean+linear/iris_student.csv', header=None)
        self.data = self.data.to_numpy()
        self.data = self.data[1:, 1:5].astype(float)

        self.dimension = len(self.data[0])  #Âá†Áª¥
        self.numofdata = len(self.data)    #data‰∏™Êï∞
        self.meanquantity = 4    #mean‰∏™
        
    def calculate_k(self):
        self.wcss_all = np.zeros([10,2])
        for x in range(1,11):
            self.meanquantity = x
            print(x)
            wcss = 0
            for a in range(50):  #ÁÆó50Ê¨°ÂèñÂπ≥ÂùáÂÄº
                self.mean = np.zeros([self.meanquantity,self.dimension])  #ÂÆö‰πâmeanÊï∞ÁªÑ
                self.random_m3()
                flag1 = True   #Âà§Êñ≠ÊòØÂê¶Áõ∏Á≠â
                while flag1 == True:
                    flag1 = False
                    self.distance()
                    self.new_mean()
                    for c in range(len(self.mean_new)):
                        for b in range(len(self.mean_new[0])):
                            if not(self.mean_new[c,b] <= self.mean[c,b] + 0.1 and self.mean_new[c,b] >= self.mean[c,b] - 0.1):
                                flag1 = True
                    self.mean = self.mean_new
                for q in range(self.meanquantity):    #ÊÄªÂÖ±Âá†‰∏™mean
                    for w in range(int(self.mean_group_count[q])):  #Ëøô‰∏ÄÁªÑÁöÑ‰∏™Êï∞
                        for e in range(int(self.dimension)):  #Áª¥Â∫¶
                            wcss += (self.mean_group[q,w,e] - self.mean[q,e])**2
                # print(a)
            self.wcss_all[x-1,0] = x
            self.wcss_all[x-1,1] = wcss/50
        self.meanquantity = 1
        difference = self.wcss_all[0,1] - self.wcss_all[9,1]
        for a in range(1,9):   #Âà§Êñ≠K
            after = self.wcss_all[a,1] - self.wcss_all[a+1,1]
            self.meanquantity = a + 1
            if after / difference <= 0.075:
                break
        print(self.meanquantity)


    def plot_wcss(self):
        plt.plot(self.wcss_all[:,0], self.wcss_all[:,1])
        plt.pause(10)
                    
                

    def random_m1(self):
        max_data = np.zeros(self.dimension)  #ÊâæÂà∞ÊúÄÂ§ßÊúÄÂ∞èÂÄº
        min_data = np.zeros(self.dimension)
        for x in range(self.dimension):  #ÂàùÂßãÂåñ
            max_data[x] = self.data[0,x]
            min_data[x] = self.data[0,x]
        for x in range(1,self.numofdata):      #ÊØîËæÉ
            for y in range(self.dimension):
                if self.data[x,y] > max_data[y]:
                    max_data[y] = self.data[x,y]
                if self.data[x,y] < min_data[y]:
                    min_data[y] = self.data[x,y]
        for x in range(self.meanquantity):     #Âá†‰∏™meanÂá†Ê¨°  ÈöèÊú∫mean
            one_mean = np.zeros(self.dimension)
            for y in range(self.dimension):   #Âá†Áª¥Âá†Ê¨°
                one_mean[y] = random()+randint(int(min_data[y]),int(max_data[y]))
            self.mean[x] = one_mean

    def random_m2(self):
        index = sample(range(self.numofdata),self.meanquantity)
        for x in range(self.meanquantity):
            self.mean[x] = self.data[index[x]]
        # print(self.mean)

    def random_m3(self):
        range1 = np.arange(self.numofdata)  #ÈöèÊú∫ÈÄâ‰∏Ä‰∏™ÁÇπ
        index = choice(range1)
        np.delete(range1,range1[index])  #‰∏çÈáçÂ§çÈÄâÁÇπ
        self.mean[0] = self.data[index]
        for x in range(1,self.meanquantity):  #Á°ÆÂÆöÂâ©‰∏ãÁöÑmeanquantity-1 ‰∏™mean
            distance = np.zeros(self.numofdata)   #ÂàùÂßãÂåñ
            probability = np.zeros(self.numofdata)
            for data in range1:       #ÈÅçÂéÜÊØè‰∏Ä‰∏™Ê≤°ÊúâË¢´ÈÄâ‰∏≠ÁöÑÊï∞ÊçÆ
                group = np.zeros(x)        #ÂàùÂßãÂåñ Âá†‰∏™meanÂá†‰∏™0
                for z in range(x):          #‰πãÂâçÊúâÁöÑmean
                    mean = self.mean[z]
                    group[z] = np.linalg.norm(data-mean)  #ÁÆóË∑ùÁ¶ª
                distance[data] = min(group)    #ÂèñÊúÄÂ§ßÂÄº ÂàÜÊï£
                # print(group)
            wcss = np.sum(distance)
            probability = distance / wcss    #ÁÆóÂá∫
            # print(distance,probability)
            index = np.random.choice(a=range1,replace=False,size=1,p=probability)
            self.mean[x] = self.data[index]
            np.delete(range1,range1[index])
        
    def distance(self):  #ÂàÜÁªÑ
        self.mean_group = np.zeros([self.meanquantity,self.numofdata,self.dimension])  #Âá†‰∏™mean‰∏∫Âá†‰∏™ Áî®‰∫éÂàÜÁªÑ
        self.mean_group_count = np.zeros(self.meanquantity)  #ÊØè‰∏ÄÁªÑgroupÁöÑÈ°π
        for data in self.data:       #ÊØè‰∏Ä‰∏™Êï∞ÊçÆ
            count = 0
            group = np.zeros(self.meanquantity)          #ÂàùÂßãÂåñ Âá†‰∏™meanÂá†‰∏™0
            for mean in self.mean:     #ÊØè‰∏Ä‰∏™Âπ≥ÂùáÊï∞
                distance = np.linalg.norm(data-mean)
                group[count] = distance
                count += 1
            ans = min(group)
            # print(group)
            index = np.where(group == ans)[0][0]
            # print(index,self.mean_group_count[index])
            self.mean_group[index,int(self.mean_group_count[index])] = data  #ÊîæÂÖ•ÂàÜÁªÑ
            self.mean_group_count[index] += 1   #‰∏ã‰∏ÄÈ°π
        # print(self.mean_group)


    def new_mean(self): #4
        self.mean_new = np.zeros([self.meanquantity,self.dimension])  #ÂÆö‰πâ‰∏Ä‰∏™Êñ∞ÁöÑmean
        mean_count = 0      #ÂàùÂßãÂåñÂà§Êñ≠ÊòØÂì™‰∏™mean
        for data_group in self.mean_group:   #ÊØè‰∏ÄÁªÑ
            total_value = np.zeros(self.dimension)  #ÂÆö‰πâÂÖ®ÈÉΩÊòØ0
            data_quantity = self.mean_group_count[mean_count]
            for data in data_group:   #Âçï‰∏™Êï∞ÊçÆ
                total_value += data
            if data_quantity != 0:
                total_value /= data_quantity
            self.mean_new[mean_count] = total_value
            mean_count += 1 #ËÆ∞ÂΩïÊòØÂì™‰∏™mean
        # print(self.mean_new,"mean")
        # print(self.mean)
        
    def plot(self):
        self.first = False
        color = ["red","green","blue","yellow","grey","black","brown","orange","scarlet","lilac","purple"]
        plt.clf()
        plt.axis("off")
        if self.first == True:
            plt.scatter(self.data[:, 0], self.data[:, 1])   #Á¨¨‰∏ÄÊ¨°ÂàùÂßãÂåñ
            self.first = False
            plt.pause(1)
        for x in range(self.meanquantity):   #‰πãÂêéÂàÜÁªÑÂàÜÈ¢úËâ≤
            plt.scatter(self.mean_group[x, 0:int(self.mean_group_count[x]), 0], self.mean_group[x, 0:int(self.mean_group_count[x]), 1], color = color[x])
        plt.scatter(self.mean[:, 0], self.mean[:, 1], color = color[-1])  #ÁîªmeanÁÇπ
        plt.show(block=False)
        plt.pause(1)

    def decision(self):    #Êìç‰Ωú
        fig = plt.figure()
        self.calculate_k()
        self.plot_wcss()
        self.mean = np.zeros([self.meanquantity,self.dimension])
        self.random_m3()
        flag1 = True   #Âà§Êñ≠ÊòØÂê¶Áõ∏Á≠â
        while flag1 == True:
            flag1 = False
            self.distance()
            self.new_mean()
            self.plot()
            for x in range(len(self.mean_new)):
                for y in range(len(self.mean_new[0])):
                    if not(self.mean_new[x,y] <= self.mean[x,y] + 0.01 and self.mean_new[x,y] >= self.mean[x,y] - 0.01):
                        flag1 = True
            self.mean = self.mean_new
        print("end")
        plt.pause(10)
    
mean = k_mean()
mean.decision()
								</code></pre>
								<h1>linear regression</h1>
								<pre><code>
									import numpy as np
import matplotlib.pyplot as plt
import random as rd
class regression():
    def __init__(self,a,regularisation):
        self.test_data = np.load('Êú∫Âô®Â≠¶‰π†/k-mean+linear/regression_data.npy')
        self.regularisation = regularisation
        self.train = self.test_data["train"]
        self.test = self.test_data["test"]
        self.learningrate = 1e-4
        self.x_num,self.x_num_use = a + 1, a + 1
        self.trainnum = len(self.train)
        self.testnum = len(self.test)
        self.trainx = self.train[:,0]
        self.trainy = self.train[:,1]
        self.testx = self.test[:,0]
        self.testy = self.test[:,1]
        self.trainLlist = np.zeros(self.x_num)
        self.testLlist = np.zeros(self.x_num)
        self.trainw = np.ones(self.x_num)
        self.endpoint = max(np.max(self.trainx),np.max(self.testx))
    def plot(self):
        color = ["red", "green", "blue", "yellow", "grey", "black", "brown", "orange", "scarlet", "lilac", "purple"]
        plt.scatter(self.trainx,self.trainy,color = "red")
        # plt.scatter(self.testx,self.testy,color = "blue")
        x = np.linspace(0,self.endpoint,100)
        y = np.zeros(100)
        count = 0
        # print(x)
        for j in x:
            ans = 0
            for z in range(self.x_num):
                ans += self.trainw[z]*j**z
            y[count] = ans
            count += 1
        plt.plot(x,y)
        plt.show()
        
    def initialise_x(self,x_Data,x_num):
        self.usex = np.zeros([self.x_num_use,x_num])   # size 2 10
        self.usex = self.usex + np.transpose(x_Data)   #boradcast  2*10   =  2 * 10 + 1 * 10
        for x in range(self.x_num_use):
            self.usex[x] = self.usex[x] ** x


    def gradientdescent(self):#
        for x in range(30000):
            self.initialise_x(self.trainx,self.trainnum)
            # print(np.matmul(np.transpose(self.usex),self.trainw))
            inner = self.trainy-np.matmul(np.transpose(self.usex),self.trainw)
            differentiation = - (1 / self.trainnum) * np.matmul(self.usex,inner) + self.regularisation * self.trainw
            self.trainw = self.trainw - self.learningrate * differentiation
            if np.sum(np.isnan(self.trainw)) > 0:
                raise ValueError("Gradient is NAN",self.x_num_use)
            if np.linalg.norm(differentiation) <= 2:
                print(x)
                break
        print(self.trainw)

    def solvingequations(self):
        self.initialise_x(self.trainx,self.trainnum)
        I = np.eye(self.x_num_use)
        p1 = np.matmul(self.usex,np.transpose(self.usex)) + self.regularisation * I #-1ÁöÑÈ°π ÔºàXXTÔºâ-1   2*2 =  2*1 1*2
        p2 = np.matmul(self.usex,self.trainy)   # XY   2*1 = 2*10 * 10*1
        # print(np.shape(p1),np.shape(p2))
        self.trainw = np.linalg.solve(p1,p2)
        print(self.trainw)

    def loss_graph(self):
        for x in range(self.x_num+1):
            print(x)
            self.x_num_use = x
            self.solvingequations()
            # self.gradientdescent()
            self.trainL = 1/(2 * self.trainnum)*np.matmul(np.transpose((self.trainy-np.matmul(np.transpose(self.usex),self.trainw))),self.trainy-np.matmul(np.transpose(self.usex),self.trainw))
            self.initialise_x(self.testx,self.testnum)
            x -= 1
            self.testL = 1/(2 * self.testnum)*np.matmul(np.transpose((self.testy-np.matmul(np.transpose(self.usex),self.trainw))),self.testy-np.transpose(np.matmul(np.transpose(self.trainw),self.usex)))
            # print(np.shape(self.trainL),np.shape(self.testL))
            self.trainLlist[x] = self.trainL
            self.testLlist[x] = self.testL
            # print(self.trainL)
        x = np.arange(self.x_num)
        # print(x)
        plt.plot(x,self.trainLlist)
        plt.plot(x,self.testLlist)
        plt.scatter(x,self.trainLlist,color = "red")
        plt.scatter(x,self.testLlist,color = "blue")
        plt.show()


aregression = regression(20,1e-6)
# aregression.gradientdescent()  #m2
aregression.solvingequations()  #m1
aregression.plot()   #showingresult

# aregression.loss_graph()  #drawgraph
								</code></pre>
							</article>
						<!-- Elements -->
							<article id="elements">
								<h2 class="major">Elements</h2>

								<section>
									<h3 class="major">Text</h3>
									<p>This is <b>bold</b> and this is <strong>strong</strong>. This is <i>italic</i> and this is <em>emphasized</em>.
									This is <sup>superscript</sup> text and this is <sub>subscript</sub> text.
									This is <u>underlined</u> and this is code: <code>for (;;) { ... }</code>. Finally, <a href="#">this is a link</a>.</p>
									<hr />
									<h2>Heading Level 2</h2>
									<h3>Heading Level 3</h3>
									<h4>Heading Level 4</h4>
									<h5>Heading Level 5</h5>
									<h6>Heading Level 6</h6>
									<hr />
									<h4>Blockquote</h4>
									<blockquote>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan faucibus. Vestibulum ante ipsum primis in faucibus lorem ipsum dolor sit amet nullam adipiscing eu felis.</blockquote>
									<h4>Preformatted</h4>
									<pre><code>i = 0;

while (!deck.isInOrder()) {
    print 'Iteration ' + i;
    deck.shuffle();
    i++;
}

print 'It took ' + i + ' iterations to sort the deck.';</code></pre>
								</section>

								<section>
									<h3 class="major">Lists</h3>

									<h4>Unordered</h4>
									<ul>
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Alternate</h4>
									<ul class="alt">
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Ordered</h4>
									<ol>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis viverra.</li>
										<li>Felis enim feugiat.</li>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis lorem.</li>
										<li>Felis enim et feugiat.</li>
									</ol>
									<h4>Icons</h4>
									<ul class="icons">
										<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
									</ul>

									<h4>Actions</h4>
									<ul class="actions">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions stacked">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Table</h3>
									<h4>Default</h4>
									<div class="table-wrapper">
										<table>
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>

									<h4>Alternate</h4>
									<div class="table-wrapper">
										<table class="alt">
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>
								</section>

								<section>
									<h3 class="major">Buttons</h3>
									<ul class="actions">
										<li><a href="#" class="button primary">Primary</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button">Default</a></li>
										<li><a href="#" class="button small">Small</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button primary icon solid fa-download">Icon</a></li>
										<li><a href="#" class="button icon solid fa-download">Icon</a></li>
									</ul>
									<ul class="actions">
										<li><span class="button primary disabled">Disabled</span></li>
										<li><span class="button disabled">Disabled</span></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Form</h3>
									<form method="post" action="#">
										<div class="fields">
											<div class="field half">
												<label for="demo-name">Name</label>
												<input type="text" name="demo-name" id="demo-name" value="" placeholder="Jane Doe" />
											</div>
											<div class="field half">
												<label for="demo-email">Email</label>
												<input type="email" name="demo-email" id="demo-email" value="" placeholder="jane@untitled.tld" />
											</div>
											<div class="field">
												<label for="demo-category">Category</label>
												<select name="demo-category" id="demo-category">
													<option value="">-</option>
													<option value="1">Manufacturing</option>
													<option value="1">Shipping</option>
													<option value="1">Administration</option>
													<option value="1">Human Resources</option>
												</select>
											</div>
											<div class="field half">
												<input type="radio" id="demo-priority-low" name="demo-priority" checked>
												<label for="demo-priority-low">Low</label>
											</div>
											<div class="field half">
												<input type="radio" id="demo-priority-high" name="demo-priority">
												<label for="demo-priority-high">High</label>
											</div>
											<div class="field half">
												<input type="checkbox" id="demo-copy" name="demo-copy">
												<label for="demo-copy">Email me a copy</label>
											</div>
											<div class="field half">
												<input type="checkbox" id="demo-human" name="demo-human" checked>
												<label for="demo-human">Not a robot</label>
											</div>
											<div class="field">
												<label for="demo-message">Message</label>
												<textarea name="demo-message" id="demo-message" placeholder="Enter your message" rows="6"></textarea>
											</div>
										</div>
										<ul class="actions">
											<li><input type="submit" value="Send Message" class="primary" /></li>
											<li><input type="reset" value="Reset" /></li>
										</ul>
									</form>
								</section>

							</article>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; WEIJUN HUANG</p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>